# TODO: wandb configuration

wandb:
  mode: online # online, offline, disabled
  project: "ViT_adaptive"
  save_code: false

scheduler:
  enabled: false
  address: http://127.0.0.1:8000

cluster_config:
  num_nodes: null
  num_devices_per_node: null
  namespace: null # alpa_default_space by default

model_name_or_path: google/vit-base-patch16-224-in21k
# model_name_or_path: google/vit-huge-patch14-224-in21k

paths:
  # output_dir:  ./vit-base-patch16-imagenette
  # train_dir: imagenette2/train
  # validation_dir: imagenette2/val
  output_dir:  ./vit-base-patch16-imagenet
  # output_dir: ./vit-huge-patch14-imagenet
  train_dir: imagenet/train
  validation_dir: imagenet/val
  cache_dir: cache

pollux_agent:
  regression_coefficients:
    - key: [1, 1]
      coef: 0.00206749
      intercept: 0.019354801896649114
    - key: [2, 1]
      coef: 0.00108904
      intercept: 0.027093764782572798
    - key: [4, 1]
      coef: 0.00056805
      intercept: 0.028780623571947224
  fix_regressors: true

dataloader:
  train:
    adaptive_data_loader:
      enabled: true
      autoscaler: 
        enabled: false
        max_total_batch_size: 800
        local_bsz_bounds:
          min_is_init_bs: true
          min: 32
          max: 150
    init_local_batch_size: 32
    shuffle: true
    preprocessing_num_workers: 32
    persistent_workers: true
  eval:
    local_batch_size: 64
    shuffle: false


training:
  dtype: "float32"
  pretrain: false
  num_train_epochs: 10000000
  learning_rate: 1e-5
  copus_enabled: false
  scale_lr:
    enabled: true
    type: sqrt # linear, sqrt
  parallel_method:
    method: DataParallel # DataParallel, ShardParallel, PipeshardParallel, 3D
    parameters:
      DataParallel:
      ShardParallel:
      PipeshardParallel:
        stage_option: uniform # uniform, auto
        # num stages?
      _3D:
        data_parallel: -1
        operator_parallel: 1
        pipeline_parallel: 1
    num_micro_batches: 1

evaluation:
  enabled: false